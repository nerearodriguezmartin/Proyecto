{"version":3,"sources":["./node_modules/@aws-amplify/ui-components/dist/esm/amplify-chatbot.entry.js"],"names":[],"mappings":";;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAA6G;AAC7C;AACrC;AACoC;AACa;AACnB;;AAEzD;AACA;AACA;AACA,sBAAsB;AACtB,6BAA6B;AAC7B,6BAA6B;AAC7B,yCAAyC;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,wBAAwB;AAC3C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kCAAkC,2CAA2C;AAC7E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,kBAAkB;AACrC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,mBAAmB;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yCAAyC;AACzC,uBAAuB;AACvB,mCAAmC;AACnC,kDAAkD;AAClD;AACA,kDAAkD;AAClD;AACA;AACA;AACA;AACA;AACA,iCAAiC;AACjC,gDAAgD;AAChD,iCAAiC;AACjC;AACA,kCAAkC;AAClC,0CAA0C;AAC1C,gCAAgC;AAChC,gCAAgC;AAChC,+CAA+C;AAC/C,mDAAmD;AACnD,gCAAgC;AAChC,iCAAiC;AACjC;AACA,kCAAkC;AAClC,wCAAwC;AACxC,uCAAuC;AACvC;AACA;AACA;AACA;AACA;AACA,qBAAqB,eAAe;AACpC,qBAAqB,OAAO;AAC5B,4BAA4B,OAAO;AACnC,4BAA4B,OAAO;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA,mBAAmB,wDAAM;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,YAAY,uEAAa;AACzB;AACA;AACA;AACA,+BAA+B,cAAc;AAC7C;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6DAA6D;AAC7D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yBAAyB,eAAe;AACxC,0BAA0B,WAAW;AACrC;AACA;AACA;AACA;AACA,mDAAmD;AACnD,qDAAqD;AACrD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qDAAqD;AACrD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB,kBAAkB;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gCAAgC,OAAO;AACvC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,SAAS,uEAAa;AACtB;AACA,WAAW,gBAAgB;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2EAA2E;AAC3E;AACA;AACA;AACA,uBAAuB,iCAAiC;AACxD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,qCAAqC,sCAAsC,WAAW,uCAAuC,cAAc,eAAe,mBAAmB,qBAAqB,eAAe,gBAAgB,oBAAoB,aAAa,oBAAoB,qDAAqD,6CAA6C,2BAA2B,mBAAmB,sBAAsB,qBAAqB,sBAAsB,4DAA4D,oDAAoD,6BAA6B,qBAAqB,qBAAqB,qDAAqD,6CAA6C,2BAA2B,mBAAmB,gCAAgC,GAAG,aAAa,SAAS,aAAa,wBAAwB,GAAG,aAAa,SAAS,aAAa,MAAM,iBAAiB,iBAAiB,8CAA8C,qCAAqC,0CAA0C,uBAAuB,sCAAsC,4CAA4C,uCAAuC,wCAAwC,iBAAiB,2BAA2B,oBAAoB,0BAA0B,sBAAsB,yCAAyC,uBAAuB,gEAAgE,wDAAwD,8BAA8B,sBAAsB,uCAAuC,mBAAmB,WAAW,qBAAqB,uBAAuB,0BAA0B,iBAAiB,oBAAoB,QAAQ,0CAA0C,0BAA0B,6BAA6B,iBAAiB,kBAAkB,qBAAqB,MAAM,+CAA+C,2BAA2B,oBAAoB,aAAa,oBAAoB,YAAY,0BAA0B,sBAAsB,cAAc,QAAQ,eAAe,oBAAoB,gBAAgB,qBAAqB,uBAAuB,KAAK,kBAAkB,6CAA6C,4BAA4B,qCAAqC,MAAM,iBAAiB,8CAA8C,6BAA6B,qCAAqC,QAAQ,oBAAoB,aAAa,sBAAsB,mBAAmB,8CAA8C,uBAAuB,oBAAoB,sBAAsB,cAAc,WAAW,oBAAoB,YAAY,OAAO,qBAAqB,sBAAsB,oBAAoB,YAAY,gBAAgB,aAAa,sBAAsB,yCAAyC,mBAAmB,aAAa;;AAEt8F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,8BAA8B;AAC/B;AACA;AACA;AACA;AACA;AACA,CAAC,kCAAkC;AACnC;AACA;AACA;AACA;AACA;AACA,CAAC,sCAAsC;AACvC;AACA;AACA,QAAQ,4DAAgB;AACxB;AACA;AACA;AACA;AACA;AACA,wBAAwB,2DAAY;AACpC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wDAAwD,4DAAC,SAAS,kBAAkB,aAAa,GAAG;AACpG;AACA;AACA;AACA,iCAAiC,4DAAC,SAAS,kBAAkB,OAAO,GAAG,EAAE,4DAAC,SAAS,wBAAwB,OAAO,GAAG,EAAE,4DAAC,UAAU,oBAAoB,GAAG,4DAAC,UAAU,sBAAsB,GAAG,4DAAC,UAAU,qBAAqB;AAC7N;AACA;AACA;AACA,6BAA6B,4DAAW;AACxC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,sEAAY,WAAW,sEAAY;AAChD,4BAA4B,wDAA4B;AACxD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0BAA0B,2DAAY;AACtC;AACA;AACA;AACA,0BAA0B,2DAAY;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,YAAY,sEAAY;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+BAA+B;AAC/B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6B,sEAAY;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA,6BAA6B,sEAAY;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sBAAsB;AACtB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8BAA8B,4DAAC,YAAY,eAAe;AAC1D,8BAA8B,4DAAC,oBAAoB,+IAA+I;AAClM;AACA;AACA;AACA;AACA;AACA;AACA,cAAc,2DAAY;AAC1B,cAAc,2DAAY;AAC1B,2BAA2B,4DAAC,mBAAmB,cAAc,sDAAI,wLAAwL;AACzP,gDAAgD,4DAAC,oBAAoB,sOAAsO;AAC3S,gDAAgD,4DAAC,oBAAoB,iOAAiO;AACtS;AACA;AACA;AACA;AACA;AACA,eAAe,qBAAqB;AACpC,eAAe,4DAAC,mBAAmB,UAAU,sDAAI,oEAAoE;AACrH;AACA;AACA,gBAAgB,4DAAC,CAAC,oDAAI,QAAQ,4DAAC,SAAS,2BAA2B,EAAE,4DAAC,UAAU,iBAAiB,EAAE,4DAAC,SAAS,iDAAiD,EAAE,sDAAI,uBAAuB,4DAAC,SAAS,6CAA6C,mCAAmC,4DAAC,SAAS,iDAAiD;AAChV;AACA,mBAAmB,QAAQ,4DAAU,OAAO;AAC5C;AACA;;AAE6C","file":"5.js","sourcesContent":["import { r as registerInstance, h, c as createEvent, H as Host, g as getElement } from './index-3fb5c139.js';\nimport { Logger, browserOrNode, I18n } from '@aws-amplify/core';\nimport '@aws-amplify/auth';\nimport { T as Translations } from './Translations-c833f663.js';\nimport { d as NO_INTERACTIONS_MODULE_FOUND } from './constants-d1abe7de.js';\nimport { Interactions } from '@aws-amplify/interactions';\n\n// AudioRecorder settings\nconst RECORDER_EXPORT_MIME_TYPE = 'application/octet-stream';\nconst DEFAULT_EXPORT_SAMPLE_RATE = 16000;\nconst FFT_SIZE = 2048; // window size in samples for Fast Fourier Transform (FFT)\nconst FFT_MAX_DECIBELS = -10; // maximum power value in the scaling range for the FFT analysis data\nconst FFT_MIN_DECIBELS = -90; // minimum power value in the scaling range for the FFT analysis data\nconst FFT_SMOOTHING_TIME_CONSTANT = 0.85; // averaging constant with the last analysis frame\n\n/**\n * Merges multiple buffers into one.\n */\nconst mergeBuffers = (bufferArray, recLength) => {\n    const result = new Float32Array(recLength);\n    let offset = 0;\n    for (let i = 0; i < bufferArray.length; i++) {\n        result.set(bufferArray[i], offset);\n        offset += bufferArray[i].length;\n    }\n    return result;\n};\n/**\n * Downsamples audio to desired export sample rate.\n */\nconst downsampleBuffer = (buffer, recordSampleRate, exportSampleRate) => {\n    if (exportSampleRate === recordSampleRate) {\n        return buffer;\n    }\n    const sampleRateRatio = recordSampleRate / exportSampleRate;\n    const newLength = Math.round(buffer.length / sampleRateRatio);\n    const result = new Float32Array(newLength);\n    let offsetResult = 0;\n    let offsetBuffer = 0;\n    while (offsetResult < result.length) {\n        const nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio);\n        let accum = 0, count = 0;\n        for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {\n            accum += buffer[i];\n            count++;\n        }\n        result[offsetResult] = accum / count;\n        offsetResult++;\n        offsetBuffer = nextOffsetBuffer;\n    }\n    return result;\n};\n/**\n * converts raw audio values to 16 bit pcm.\n */\nconst floatTo16BitPCM = (output, offset, input) => {\n    let byteOffset = offset;\n    for (let i = 0; i < input.length; i++, byteOffset += 2) {\n        const s = Math.max(-1, Math.min(1, input[i]));\n        output.setInt16(byteOffset, s < 0 ? s * 0x8000 : s * 0x7fff, true);\n    }\n};\n/**\n * Write given strings in big-endian order.\n */\nconst writeString = (view, offset, string) => {\n    for (let i = 0; i < string.length; i++) {\n        view.setUint8(offset + i, string.charCodeAt(i));\n    }\n};\n/**\n * Encodes raw pcm audio into a wav file.\n */\nconst encodeWAV = (samples, exportSampleRate) => {\n    /**\n     * WAV file consists of three parts: RIFF header, WAVE subchunk, and data subchunk. We precompute the size of them.\n     */\n    const audioSize = samples.length * 2; // We use 16-bit samples, so we have (2 * sampleLength) bytes.\n    const fmtSize = 24; // Byte size of the fmt subchunk: 24 bytes that the audio information that we'll set below.\n    const dataSize = 8 + audioSize; // Byte size of the data subchunk: raw sound data plus 8 bytes for the subchunk descriptions.\n    const totalByteSize = 12 + fmtSize + dataSize; // Byte size of the whole file, including the chunk header / descriptor.\n    // create DataView object to write byte values into\n    const buffer = new ArrayBuffer(totalByteSize); // buffer to write the chunk values in.\n    const view = new DataView(buffer);\n    /**\n     * Start writing the .wav file. We write top to bottom, so byte offset (first numeric argument) increases strictly.\n     */\n    // RIFF header\n    writeString(view, 0, 'RIFF'); // At offset 0, write the letters \"RIFF\"\n    view.setUint32(4, fmtSize + dataSize, true); // At offset 4, write the size of fmt and data chunk size combined.\n    writeString(view, 8, 'WAVE'); // At offset 8, write the format type \"WAVE\"\n    // fmt subchunk\n    writeString(view, 12, 'fmt '); //chunkdId 'fmt '\n    view.setUint32(16, fmtSize - 8, true); // fmt subchunk size below this value. We set 8 bytes already, so subtract 8 bytes from fmtSize.\n    view.setUint16(20, 1, true); // Audio format code, which is 1 for PCM.\n    view.setUint16(22, 1, true); // Number of audio channels. We use mono, ie 1.\n    view.setUint32(24, exportSampleRate, true); // Sample rate of the audio file.\n    view.setUint32(28, exportSampleRate * 2, true); // Data rate, or # of data bytes per second. Since each sample is 2 bytes, this is 2 * sampleRate.\n    view.setUint16(32, 2, true); // block align, # of bytes per sample including all channels, ie. 2 bytes.\n    view.setUint16(34, 16, true); // bits per sample, ie. 16 bits\n    // data subchunk\n    writeString(view, 36, 'data'); // write the chunkId 'data'\n    view.setUint32(40, audioSize, true); // Audio byte size\n    floatTo16BitPCM(view, 44, samples); // raw pcm values then go here.\n    return view;\n};\n/**\n * Given arrays of raw pcm audio, downsamples the audio to desired sample rate and encodes it to a wav audio file.\n *\n * @param recBuffer {Float32Array[]} - 2d float array containing the recorded raw audio\n * @param recLength {number} - total length of recorded audio\n * @param recordSampleRate {number} - sample rate of the recorded audio\n * @param exportSampleRate {number} - desired sample rate of the exported file\n */\nconst exportBuffer = (recBuffer, recLength, recordSampleRate, exportSampleRate) => {\n    const mergedBuffers = mergeBuffers(recBuffer, recLength);\n    const downsampledBuffer = downsampleBuffer(mergedBuffers, recordSampleRate, exportSampleRate);\n    const encodedWav = encodeWAV(downsampledBuffer, exportSampleRate);\n    const audioBlob = new Blob([encodedWav], {\n        type: RECORDER_EXPORT_MIME_TYPE,\n    });\n    return audioBlob;\n};\n\nconst logger = new Logger('AudioRecorder');\nclass AudioRecorder {\n    constructor(options) {\n        // input mic stream is stored in a buffer\n        this.streamBuffer = [];\n        this.streamBufferLength = 0;\n        this.recording = false;\n        this.options = options;\n    }\n    /**\n     * This must be called first to enable audio context and request microphone access.\n     * Once access granted, it connects all the necessary audio nodes to the context so that it can begin recording or playing.\n     */\n    async init() {\n        if (browserOrNode().isBrowser) {\n            window.AudioContext = window.AudioContext || window.webkitAudioContext;\n            this.audioContext = new AudioContext();\n            await navigator.mediaDevices\n                .getUserMedia({ audio: true })\n                .then(stream => {\n                this.audioSupported = true;\n                this.setupAudioNodes(stream);\n            })\n                .catch(() => {\n                this.audioSupported = false;\n                return Promise.reject('Audio is not supported');\n            });\n        }\n        else {\n            this.audioSupported = false;\n            return Promise.reject('Audio is not supported');\n        }\n    }\n    /**\n     * Setup audio nodes after successful `init`.\n     */\n    async setupAudioNodes(stream) {\n        try {\n            await this.audioContext.resume();\n        }\n        catch (err) {\n            logger.error(err);\n        }\n        const sourceNode = this.audioContext.createMediaStreamSource(stream);\n        const processorNode = this.audioContext.createScriptProcessor(4096, 1, 1);\n        processorNode.onaudioprocess = audioProcessingEvent => {\n            if (!this.recording)\n                return;\n            const stream = audioProcessingEvent.inputBuffer.getChannelData(0);\n            this.streamBuffer.push(new Float32Array(stream)); // set to a copy of the stream\n            this.streamBufferLength += stream.length;\n            this.analyse();\n        };\n        const analyserNode = this.audioContext.createAnalyser();\n        analyserNode.minDecibels = FFT_MIN_DECIBELS;\n        analyserNode.maxDecibels = FFT_MAX_DECIBELS;\n        analyserNode.smoothingTimeConstant = FFT_SMOOTHING_TIME_CONSTANT;\n        sourceNode.connect(analyserNode);\n        analyserNode.connect(processorNode);\n        processorNode.connect(sourceNode.context.destination);\n        this.analyserNode = analyserNode;\n    }\n    /**\n     * Start recording audio and listen for silence.\n     *\n     * @param onSilence {SilenceHandler} - called whenever silence is detected\n     * @param visualizer {Visualizer} - called with audio data on each audio process to be used for visualization.\n     */\n    async startRecording(onSilence, visualizer) {\n        if (this.recording || !this.audioSupported)\n            return;\n        this.onSilence = onSilence || function () { };\n        this.visualizer = visualizer || function () { };\n        const context = this.audioContext;\n        try {\n            await context.resume();\n        }\n        catch (err) {\n            logger.error(err);\n        }\n        this.start = Date.now();\n        this.recording = true;\n    }\n    /**\n     * Pause recording\n     */\n    stopRecording() {\n        if (!this.audioSupported)\n            return;\n        this.recording = false;\n    }\n    /**\n     * Pause recording and clear audio buffer\n     */\n    clear() {\n        this.stopRecording();\n        this.streamBufferLength = 0;\n        this.streamBuffer = [];\n    }\n    /**\n     * Plays given audioStream with audioContext\n     *\n     * @param buffer {Uint8Array} - audioStream to be played\n     */\n    play(buffer) {\n        if (!buffer || !this.audioSupported)\n            return;\n        const myBlob = new Blob([buffer]);\n        return new Promise((res, rej) => {\n            const fileReader = new FileReader();\n            fileReader.onload = () => {\n                if (this.playbackSource)\n                    this.playbackSource.disconnect(); // disconnect previous playback source\n                this.playbackSource = this.audioContext.createBufferSource();\n                const successCallback = (buf) => {\n                    this.playbackSource.buffer = buf;\n                    this.playbackSource.connect(this.audioContext.destination);\n                    this.playbackSource.onended = () => {\n                        return res();\n                    };\n                    this.playbackSource.start(0);\n                };\n                const errorCallback = err => {\n                    return rej(err);\n                };\n                this.audioContext.decodeAudioData(fileReader.result, successCallback, errorCallback);\n            };\n            fileReader.onerror = () => rej();\n            fileReader.readAsArrayBuffer(myBlob);\n        });\n    }\n    /**\n     * Stops playing audio if there's a playback source connected.\n     */\n    stop() {\n        if (this.playbackSource) {\n            this.playbackSource.stop();\n        }\n    }\n    /**\n     * Called after each audioProcess. Check for silence and give fft time domain data to visualizer.\n     */\n    analyse() {\n        if (!this.audioSupported)\n            return;\n        const analyser = this.analyserNode;\n        analyser.fftSize = FFT_SIZE;\n        const bufferLength = analyser.fftSize;\n        const dataArray = new Uint8Array(bufferLength);\n        const amplitude = this.options.amplitude;\n        const time = this.options.time;\n        analyser.getByteTimeDomainData(dataArray);\n        this.visualizer(dataArray, bufferLength);\n        for (let i = 0; i < bufferLength; i++) {\n            // Normalize between -1 and 1.\n            const curr_value_time = dataArray[i] / 128 - 1.0;\n            if (curr_value_time > amplitude || curr_value_time < -1 * amplitude) {\n                this.start = Date.now();\n            }\n        }\n        const newtime = Date.now();\n        const elapsedTime = newtime - this.start;\n        if (elapsedTime > time) {\n            this.onSilence();\n        }\n    }\n    /**\n     * Encodes recorded buffer to a wav file and exports it to a blob.\n     *\n     * @param exportSampleRate {number} - desired sample rate of the exported buffer\n     */\n    async exportWAV(exportSampleRate = DEFAULT_EXPORT_SAMPLE_RATE) {\n        if (!this.audioSupported)\n            return;\n        const recordSampleRate = this.audioContext.sampleRate;\n        const blob = exportBuffer(this.streamBuffer, this.streamBufferLength, recordSampleRate, exportSampleRate);\n        this.clear();\n        return blob;\n    }\n}\n\nconst visualize = (dataArray, bufferLength, canvas) => {\n    if (!canvas)\n        return;\n    if (!browserOrNode().isBrowser)\n        throw new Error('Visualization is not supported on non-browsers.');\n    const { width, height } = canvas.getBoundingClientRect();\n    // need to update the default canvas width and height\n    canvas.width = width;\n    canvas.height = height;\n    const canvasCtx = canvas.getContext('2d');\n    canvasCtx.fillStyle = 'white';\n    canvasCtx.clearRect(0, 0, width, height);\n    const draw = () => {\n        canvasCtx.fillRect(0, 0, width, height);\n        canvasCtx.lineWidth = 1;\n        const color = getComputedStyle(document.documentElement).getPropertyValue('--amplify-primary-color');\n        canvasCtx.strokeStyle = !color || color === '' ? '#ff9900' : color; // TODO: try separate css variable\n        canvasCtx.beginPath();\n        const sliceWidth = (width * 1.0) / bufferLength;\n        let x = 0;\n        for (let i = 0; i < bufferLength || i % 3 === 0; i++) {\n            const value = dataArray[i] / 128.0;\n            const y = (value * height) / 2;\n            if (i === 0) {\n                canvasCtx.moveTo(x, y);\n            }\n            else {\n                canvasCtx.lineTo(x, y);\n            }\n            x += sliceWidth;\n        }\n        canvasCtx.lineTo(canvas.width, canvas.height / 2);\n        canvasCtx.stroke();\n    };\n    // Register our draw function with requestAnimationFrame.\n    requestAnimationFrame(draw);\n};\n\nconst amplifyChatbotCss = \".bot .dot{background-color:var(--bot-dot-color)}.user .dot{background-color:var(--user-dot-color)}.dot-flashing{width:2.625rem}.dot-flashing .dot{display:inline-block;width:0.625rem;height:0.625rem;border-radius:10rem;opacity:0.65}.dot-flashing .left{-webkit-animation:dot-flashing 1s infinite alternate;animation:dot-flashing 1s infinite alternate;-webkit-animation-delay:0s;animation-delay:0s}.dot-flashing .middle{margin-left:0.375rem;margin-right:0.375rem;-webkit-animation:dot-flashing 1s infinite linear alternate;animation:dot-flashing 1s infinite linear alternate;-webkit-animation-delay:0.5s;animation-delay:0.5s}.dot-flashing .right{-webkit-animation:dot-flashing 1s infinite alternate;animation:dot-flashing 1s infinite alternate;-webkit-animation-delay:1s;animation-delay:1s}@-webkit-keyframes dot-flashing{0%{opacity:0.65}50%,100%{opacity:0.1}}@keyframes dot-flashing{0%{opacity:0.65}50%,100%{opacity:0.1}}:host{--width:28.75rem;--height:37.5rem;--header-color:var(--amplify-secondary-color);--header-size:var(--amplify-text-lg);--bot-background-color:rgb(230, 230, 230);--bot-text-color:black;--bot-dot-color:var(--bot-text-color);--user-background-color:var(--amplify-blue);--user-text-color:var(--amplify-white);--user-dot-color:var(--user-text-color)}.amplify-chatbot{display:-ms-inline-flexbox;display:inline-flex;-ms-flex-direction:column;flex-direction:column;background-color:var(--background-color);border-radius:0.375rem;-webkit-box-shadow:0.0625rem 0rem 0.25rem 0 rgba(0, 0, 0, 0.15);box-shadow:0.0625rem 0rem 0.25rem 0 rgba(0, 0, 0, 0.15);-webkit-box-sizing:border-box;box-sizing:border-box;font-family:var(--amplify-font-family);margin-bottom:1rem;width:100%;height:var(--height);max-width:var(--width)}@media (min-width: 672px){.amplify-chatbot{width:var(--width)}}.header{padding:1.25rem 0.375rem 1.25rem 0.375rem;color:var(--header-color);font-size:var(--header-size);font-weight:bold;text-align:center;word-wrap:break-word}.body{border-top:0.0625rem solid rgba(0, 0, 0, 0.05);padding:1.5rem 1rem 0 1rem;display:-ms-flexbox;display:flex;-ms-flex-positive:1;flex-grow:1;-ms-flex-direction:column;flex-direction:column;overflow:auto}.bubble{max-width:100%;padding:0.8em 1.4em;text-align:left;word-wrap:break-word;margin-bottom:0.625rem}.bot{margin-right:auto;background-color:var(--bot-background-color);color:var(--bot-text-color);border-radius:1.5rem 1.5rem 1.5rem 0}.user{margin-left:auto;background-color:var(--user-background-color);color:var(--user-text-color);border-radius:1.5rem 1.5rem 0 1.5rem}.footer{display:-ms-flexbox;display:flex;-ms-flex-align:center;align-items:center;border-top:0.062rem solid rgba(0, 0, 0, 0.05);padding-right:0.625rem;min-height:3.125rem}.footer amplify-input{--border:none;--margin:0;-ms-flex-positive:1;flex-grow:1}canvas{margin-left:0.625rem;margin-right:0.625rem;-ms-flex-positive:1;flex-grow:1;height:3.125rem}.icon-button{--icon-height:1.25rem;--icon-fill:var(--amplify-primary-color);--padding:0.625rem;--width:auto}\";\n\n// enum for possible bot states\nvar ChatState;\n(function (ChatState) {\n    ChatState[ChatState[\"Initial\"] = 0] = \"Initial\";\n    ChatState[ChatState[\"Listening\"] = 1] = \"Listening\";\n    ChatState[ChatState[\"SendingText\"] = 2] = \"SendingText\";\n    ChatState[ChatState[\"SendingVoice\"] = 3] = \"SendingVoice\";\n    ChatState[ChatState[\"Error\"] = 4] = \"Error\";\n})(ChatState || (ChatState = {}));\n// Message types\nvar MessageFrom;\n(function (MessageFrom) {\n    MessageFrom[\"Bot\"] = \"bot\";\n    MessageFrom[\"User\"] = \"user\";\n})(MessageFrom || (MessageFrom = {}));\n// Error types\nvar ChatErrorType;\n(function (ChatErrorType) {\n    ChatErrorType[ChatErrorType[\"Recoverable\"] = 0] = \"Recoverable\";\n    ChatErrorType[ChatErrorType[\"Unrecoverable\"] = 1] = \"Unrecoverable\";\n})(ChatErrorType || (ChatErrorType = {}));\nconst AmplifyChatbot = class {\n    constructor(hostRef) {\n        registerInstance(this, hostRef);\n        /** Clear messages when conversation finishes */\n        this.clearOnComplete = false;\n        /** Continue listening to users after they send the message */\n        this.conversationModeOn = false;\n        /** Text placed in the top header */\n        this.botTitle = Translations.CHATBOT_TITLE;\n        /** Whether voice chat is enabled */\n        this.voiceEnabled = false;\n        /** Whether text chat is enabled */\n        this.textEnabled = true;\n        /** Amount of silence (in ms) to wait for */\n        this.silenceTime = 1500;\n        /** Noise threshold between -1 and 1. Anything below is considered a silence. */\n        this.silenceThreshold = 0.2;\n        /** Messages in current session */\n        this.messages = [];\n        /** Text input box value  */\n        this.text = '';\n        /** Current app state */\n        this.chatState = ChatState.Initial;\n        /**\n         * Rendering methods\n         */\n        this.messageJSX = (messages) => {\n            const messageList = messages.map(message => h(\"div\", { class: `bubble ${message.from}` }, message.content));\n            if (this.chatState === ChatState.SendingText || this.chatState === ChatState.SendingVoice) {\n                // if waiting for voice message, show animation on user side because app is waiting for transcript. Else put it on bot side.\n                const client = this.chatState === ChatState.SendingText ? MessageFrom.Bot : MessageFrom.User;\n                messageList.push(h(\"div\", { class: `bubble ${client}` }, h(\"div\", { class: `dot-flashing ${client}` }, h(\"span\", { class: \"dot left\" }), h(\"span\", { class: \"dot middle\" }), h(\"span\", { class: \"dot right\" }))));\n            }\n            return messageList;\n        };\n        this.chatCompleted = createEvent(this, \"chatCompleted\", 7);\n    }\n    // Occurs when user presses enter in input box\n    submitHandler(_event) {\n        this.sendTextMessage();\n    }\n    /**\n     * Lifecycle functions\n     */\n    componentWillLoad() {\n        if (!Interactions || typeof Interactions.onComplete !== 'function') {\n            throw new Error(NO_INTERACTIONS_MODULE_FOUND);\n        }\n        this.validateProps();\n    }\n    componentDidRender() {\n        // scroll to the bottom if necessary\n        const body = this.element.shadowRoot.querySelector('.body');\n        body.scrollTop = body.scrollHeight;\n    }\n    validateProps() {\n        if (!this.voiceEnabled && !this.textEnabled) {\n            this.setError(Translations.CHAT_DISABLED_ERROR, ChatErrorType.Unrecoverable);\n            return;\n        }\n        else if (!this.botName) {\n            this.setError(Translations.NO_BOT_NAME_ERROR, ChatErrorType.Unrecoverable);\n            return;\n        }\n        if (this.welcomeMessage)\n            this.appendToChat(this.welcomeMessage, MessageFrom.Bot);\n        // Initialize AudioRecorder if voice is enabled\n        if (this.voiceEnabled) {\n            this.audioRecorder = new AudioRecorder({\n                time: this.silenceTime,\n                amplitude: this.silenceThreshold,\n            });\n            this.audioRecorder.init().catch(err => {\n                this.setError(err, ChatErrorType.Recoverable);\n            });\n        }\n        // Callback function to be called after chat is completed\n        const onComplete = (err, data) => {\n            this.chatCompleted.emit({\n                data,\n                err,\n            });\n            if (this.clearOnComplete) {\n                this.reset();\n            }\n            else {\n                this.chatState = ChatState.Initial;\n            }\n        };\n        try {\n            Interactions.onComplete(this.botName, onComplete);\n        }\n        catch (err) {\n            this.setError(err, ChatErrorType.Unrecoverable);\n        }\n    }\n    /**\n     * Handlers\n     */\n    handleMicButton() {\n        if (this.chatState !== ChatState.Initial)\n            return;\n        this.audioRecorder.stop();\n        this.chatState = ChatState.Listening;\n        this.audioRecorder.startRecording(() => this.handleSilence(), (data, length) => this.visualizer(data, length));\n    }\n    handleSilence() {\n        this.chatState = ChatState.SendingVoice;\n        this.audioRecorder.stopRecording();\n        this.audioRecorder.exportWAV().then(blob => {\n            this.sendVoiceMessage(blob);\n        });\n    }\n    handleTextChange(event) {\n        const target = event.target;\n        this.text = target.value;\n    }\n    handleCancelButton() {\n        this.audioRecorder.clear();\n        this.chatState = ChatState.Initial;\n    }\n    handleToastClose(errorType) {\n        this.error = undefined; // clear error\n        // if error is recoverable, reset the app state to initial\n        if (errorType === ChatErrorType.Recoverable) {\n            this.chatState = ChatState.Initial;\n        }\n    }\n    /**\n     * Visualization\n     */\n    visualizer(dataArray, bufferLength) {\n        const canvas = this.element.shadowRoot.querySelector('canvas');\n        visualize(dataArray, bufferLength, canvas);\n    }\n    /**\n     * Interactions helpers\n     */\n    async sendTextMessage() {\n        if (this.text.length === 0 || this.chatState !== ChatState.Initial)\n            return;\n        const text = this.text;\n        this.text = '';\n        this.appendToChat(text, MessageFrom.User);\n        this.chatState = ChatState.SendingText;\n        let response;\n        try {\n            response = await Interactions.send(this.botName, text);\n        }\n        catch (err) {\n            this.setError(err, ChatErrorType.Recoverable);\n            return;\n        }\n        if (response.message) {\n            this.appendToChat(response.message, MessageFrom.Bot);\n        }\n        this.chatState = ChatState.Initial;\n    }\n    async sendVoiceMessage(audioInput) {\n        const interactionsMessage = {\n            content: audioInput,\n            options: {\n                messageType: 'voice',\n            },\n        };\n        let response;\n        try {\n            response = await Interactions.send(this.botName, interactionsMessage);\n        }\n        catch (err) {\n            this.setError(err, ChatErrorType.Recoverable);\n            return;\n        }\n        this.chatState = ChatState.Initial;\n        const dialogState = response.dialogState;\n        if (response.inputTranscript)\n            this.appendToChat(response.inputTranscript, MessageFrom.User);\n        this.appendToChat(response.message, MessageFrom.Bot);\n        await this.audioRecorder\n            .play(response.audioStream)\n            .then(() => {\n            // if conversationMode is on, chat is incomplete, and mic button isn't pressed yet, resume listening.\n            if (this.conversationModeOn &&\n                dialogState !== 'Fulfilled' &&\n                dialogState !== 'Failed' &&\n                this.chatState === ChatState.Initial) {\n                this.handleMicButton();\n            }\n        })\n            .catch(err => this.setError(err, ChatErrorType.Recoverable));\n    }\n    appendToChat(content, from) {\n        this.messages = [\n            ...this.messages,\n            {\n                content,\n                from,\n            },\n        ];\n    }\n    /**\n     * State control methods\n     */\n    setError(error, errorType) {\n        const message = typeof error === 'string' ? error : error.message;\n        this.chatState = ChatState.Error;\n        this.error = { message, errorType };\n    }\n    reset() {\n        this.chatState = ChatState.Initial;\n        this.text = '';\n        this.error = undefined;\n        this.messages = [];\n        if (this.welcomeMessage)\n            this.appendToChat(this.welcomeMessage, MessageFrom.Bot);\n        this.audioRecorder && this.audioRecorder.clear();\n    }\n    listeningFooterJSX() {\n        const visualization = h(\"canvas\", { height: \"50\" });\n        const cancelButton = (h(\"amplify-button\", { \"data-test\": \"chatbot-cancel-button\", handleButtonClick: () => this.handleCancelButton(), class: \"icon-button\", variant: \"icon\", icon: \"ban\" }));\n        return [visualization, cancelButton];\n    }\n    footerJSX() {\n        if (this.chatState === ChatState.Listening)\n            return this.listeningFooterJSX();\n        const inputPlaceholder = this.textEnabled\n            ? Translations.TEXT_INPUT_PLACEHOLDER\n            : Translations.VOICE_INPUT_PLACEHOLDER;\n        const textInput = (h(\"amplify-input\", { placeholder: I18n.get(inputPlaceholder), description: \"text\", handleInputChange: evt => this.handleTextChange(evt), value: this.text, disabled: this.chatState === ChatState.Error || !this.textEnabled }));\n        const micButton = this.voiceEnabled && (h(\"amplify-button\", { \"data-test\": \"chatbot-mic-button\", handleButtonClick: () => this.handleMicButton(), class: \"icon-button\", variant: \"icon\", icon: \"microphone\", disabled: this.chatState === ChatState.Error || this.chatState !== ChatState.Initial }));\n        const sendButton = this.textEnabled && (h(\"amplify-button\", { \"data-test\": \"chatbot-send-button\", class: \"icon-button\", variant: \"icon\", icon: \"send\", handleButtonClick: () => this.sendTextMessage(), disabled: this.chatState === ChatState.Error || this.chatState !== ChatState.Initial }));\n        return [textInput, micButton, sendButton];\n    }\n    errorToast() {\n        if (!this.error)\n            return;\n        const { message, errorType } = this.error;\n        return h(\"amplify-toast\", { message: I18n.get(message), handleClose: () => this.handleToastClose(errorType) });\n    }\n    render() {\n        return (h(Host, null, h(\"div\", { class: \"amplify-chatbot\" }, h(\"slot\", { name: \"header\" }, h(\"div\", { class: \"header\", \"data-test\": \"chatbot-header\" }, I18n.get(this.botTitle))), h(\"div\", { class: \"body\", \"data-test\": \"chatbot-body\" }, this.messageJSX(this.messages)), h(\"div\", { class: \"footer\", \"data-test\": \"chatbot-footer\" }, this.footerJSX()), this.errorToast())));\n    }\n    get element() { return getElement(this); }\n};\nAmplifyChatbot.style = amplifyChatbotCss;\n\nexport { AmplifyChatbot as amplify_chatbot };\n"],"sourceRoot":"webpack:///"}